
benchmark with 1000 elements is not working.


for storage, we are computing multi-exponents for the same Gs every time. so we can pre-compute multiples of each G to make this faster. pre-computing the first part of the bucket algorithm.
wrote a first draft in store:precomputed_multi_exponents, but it still doesn't work.



maybe stems should store their data as lists, because in store.erl we only access the data in order.

there are a couple todo notes in store.erl

we need to find a way to measure which parts of store are slowing us down.


it gets slow if we do more than 40k in a batch
a todo note in get.erl and in multiprove.


We need a more secure way to generate the 512 base points.
Maybe use the integers [1,2,3...] as entropy to a deterministic generator.
